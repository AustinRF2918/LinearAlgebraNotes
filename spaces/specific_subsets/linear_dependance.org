* The Definition Of *Linear Independence*
   #+BEGIN_SRC latex
    \begin{equation}c_1\vec{v_1} + c_2\vec{v_2} + ... + c_p\vec{v_p} = 0\end{equation} 
   #+END_SRC
   
   What does this mean exactly? It means we take the summation of *all the desired*
   *vectors* that we have, and give then associated *scalar weights*. Let's make 
   a theoretical Mathematica equation to represent 3 vectors and a generator 
   to make a test for such an idea:

   #+BEGIN_SRC Mathematica
     (* Define three vectors v1, v2, v3 here *)
     IsZeroVector[w1_, w2_, w3_] :=  ( w1 v1 + w2 v2 + w3 v3 ) == { 0, ... , 0 }
     isZeroVector[1, 1, 1]
     (* May output either true or false *)
   #+END_SRC
   
   In such a function, our set of vectors are deemed to be linearly independent
   if, and only if, the *ONLY* input that gets us a true evaluation is as follows

   #+BEGIN_SRC Mathematica
     isZeroVector[0, 0, 0]
   #+END_SRC
   
   Aka, only the trivial values (and naturally, zero vectors).
   
* The Definition Of *Linear Dependence*
  A set of vectors is linearly dependant, on the other hand, if and only if 
  we have some set of scalars we can plug in to our theoretical function that
  gets us out the zero vectors: see:

   #+BEGIN_SRC Mathematica
     isZeroVector[1, 2, 3]
     (* Let's say this outputs true. *)
   #+END_SRC
   
   In the case that such an evaluation happens, we can rest assured that our
   set of vectors that we have defined are linearly dependant.

* Extension To A Base Case
  This idea actually extends to a base case: Lets consider that we have only
  one vector. In such a case we may build a set from said single vector, a
  set with one item. When is such a set going to be linearly independant
  for such a function:

    #+BEGIN_SRC Mathematica
     (* Define three vectors v1, v2, v3 here *)
     IsZeroVector[w1_] :=   w1*v1 == { 0, ... , 0 }
    #+END_SRC
    
* Recursive Definition Of Linear Dependance
  Lets consider an ordered set of vectors in which length[set] > 1,
  so long as the first vector isn't a zero vector, said set will be
  linear dependance if for any item n in our set is a linear combination
  of the reverse rest of the ordered set.

* Difference Between Linear Dependance in Real Dimensions and General Dimensions.
  In general vector space, the vectors are not simple primitive tuples. This 
  means, in essence, that a homogenous equation ( f[...] == 0 ) cannot
  be written as a system of n linear equations. *We cannot reify each*
  *vector into a matrix.*

* Example From Book
  Let's consider a higher order function PolynomiaGenerator[n]. PolynomialGenerator
  takes an integer value n and outpus a function taking parameter "t". This looks like
  so. (Note this is not real Haskell)

  #+BEGIN_SRC Haskell
    PolynomialGenerator :: Integer -> Polynomial
    PolynomialGenerator 1 = 1
    PolynomialGenerator 2 = t
    PolynomialGenerator 3 = 1 - t
  #+END_SRC
  
  Now note that we can express PolynomialGenerator 3 like so:

  #+BEGIN_SRC Haskell
    PolynomialGenerator 3 = PolynomialGenerator 1 - PolynomialGenerator 2
  #+END_SRC
  
  This shows exactly how the recursive definition should be used: Polynomials are more
  complex than simple tuples: We can't represent them as simple linear equations. This
  means we have to expand out the base cases and relate them to other items in the set.
